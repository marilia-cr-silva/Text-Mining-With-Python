{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3XO86iKyh7Q"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu9YCicx65Wg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Important remarks:\n",
    "1. If you use Google Colab, upload the files to the \"content\" folder\n",
    "2. Sometimes the Colab points out an error that doesn't exist - at the dataframe target. Thus,\n",
    "if it happens, please, run again, so that the code works as expected without errors.\n",
    "'''\n",
    "\n",
    "# Libraries\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "# NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "\n",
    "# Emoji library and function\n",
    "\n",
    "!pip install demoji-0.3.0-py2.py3-none-any.whl #downloaded at https://pypi.org/project/demoji/#files\n",
    "tar = tarfile.open(\"demoji-0.3.0.tar.gz\", \"r:gz\") #downloaded at https://pypi.org/project/demoji/#files\n",
    "for member in tar.getmembers():\n",
    "     f = tar.extractfile(member)\n",
    "     if f is not None:\n",
    "         content = f.read()\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7a3jVqrjacw"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uCqXKKgK_ZJW"
   },
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "def pre_process_BERT(dataframe, file_name):\n",
    "\n",
    "    file_name = \"%s.xlsx\" % file_name\n",
    "    target = dataframe['target']\n",
    "    del dataframe['target']\n",
    "    lines, columns = dataframe.shape\n",
    "\n",
    "    if columns != 1:\n",
    "      dataframe_first = dataframe['sentence']\n",
    "      del dataframe['sentence']\n",
    "      array = np.array(dataframe)\n",
    "\n",
    "      for j in range(0,(columns-1),2):\n",
    "        for line in range(lines):\n",
    "          string = str(array[line][j])\n",
    "\n",
    "          if string == 'nan':\n",
    "            string = '??'\n",
    "            array[line][j] = string\n",
    "\n",
    "          if string != 'nan' and string != '??':\n",
    "            if array[line][j] != '':\n",
    "              string = str(array[line][j])\n",
    "              array_tokens = string.split()\n",
    "              final_list = []\n",
    "              aux = []\n",
    "          \n",
    "            for i in range(len(array_tokens)):\n",
    "              if len(array_tokens) != 0:\n",
    "                aux = str(array_tokens[i])\n",
    "\n",
    "                if aux != '' and aux != '??':\n",
    "                  # it removes URLs      \n",
    "                  aux_URL = re.sub(r'(https?:(\\/\\/))?(www\\.)?[a-zA-Z0–9@:%._\\+~#=]{1,100}\\.[a-zA-Z0–9]{2,15}\\/?\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', aux)\n",
    "                  if aux_URL != aux:\n",
    "                    aux = ''\n",
    "                  else:\n",
    "                    aux = aux_URL\n",
    "\n",
    "                  # it removes emojis - https://pypi.org/project/demoji/\n",
    "                  if aux != '' and aux != '??':\n",
    "                    a = demoji.findall(aux)\n",
    "                    size = len(aux)\n",
    "                    if a != '{}': # if there are emojis\n",
    "                      list_emojis = [] \n",
    "                      for key in a.keys():\n",
    "                        aux = re.sub(key, '', aux)\n",
    "                      if aux == '':\n",
    "                        array_tokens[i] = aux\n",
    "\n",
    "                  # it removes '#'\n",
    "                  if aux != '' and aux != '??': \n",
    "                    aux = re.sub(r'#', '', aux)\n",
    "\n",
    "                  # it removes mentions\n",
    "                  if aux != '' and aux != '??':\n",
    "                    if '@' == aux[0]:\n",
    "                      aux = ''\n",
    "                    elif '@' in aux:\n",
    "                      counter = aux.count('@')\n",
    "                      for k in range(counter): \n",
    "                        aux = re.sub(r'\\@', 'a', aux)\n",
    "\n",
    "                  if aux != '':\n",
    "                    final_list.append(aux)\n",
    "\n",
    "                  final_string = \" \".join(final_list)\n",
    "                  array[line][j] = final_string\n",
    "\n",
    "    if columns == 1:\n",
    "      dataframe_first = dataframe['sentence']\n",
    "      array = np.array(dataframe)\n",
    "\n",
    "      for j in range(0,(columns),2):\n",
    "        for line in range(lines):\n",
    "          string = str(array[line][j])\n",
    "          if string == 'nan':\n",
    "            string = '??'\n",
    "            array[line][j] = string\n",
    "\n",
    "          if string != 'nan' and string != '??':\n",
    "            if array[line][j] != '':\n",
    "              string = str(array[line][j])\n",
    "              array_tokens = string.split()\n",
    "              final_list = []\n",
    "              aux = []\n",
    "          \n",
    "            for i in range(len(array_tokens)):\n",
    "              if len(array_tokens) != 0:\n",
    "                aux = str(array_tokens[i])\n",
    "\n",
    "                if aux != '' and aux != '??':\n",
    "                  # it removes URLs      \n",
    "                  aux_URL = re.sub(r'(https?:(\\/\\/))?(www\\.)?[a-zA-Z0–9@:%._\\+~#=]{1,100}\\.[a-zA-Z0–9]{2,15}\\/?\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', aux)\n",
    "                  if aux_URL != aux:\n",
    "                    aux = ''\n",
    "                  else:\n",
    "                    aux = aux_URL\n",
    "\n",
    "                  # it removes emojis - https://pypi.org/project/demoji/\n",
    "                  if aux != '' and aux != '??':\n",
    "                    a = demoji.findall(aux)\n",
    "                    size = len(aux)\n",
    "                    if a != '{}': # if there are emojis\n",
    "                      list_emojis = [] \n",
    "                      for key in a.keys():\n",
    "                        aux = re.sub(key, '', aux)\n",
    "                      if aux == '':\n",
    "                        array_tokens[i] = aux\n",
    "\n",
    "                  # it removes '#'\n",
    "                  if aux != '' and aux != '??': \n",
    "                    aux = re.sub(r'#', '', aux)\n",
    "\n",
    "                  # it removes mentions\n",
    "                  if aux != '' and aux != '??':\n",
    "                    if '@' == aux[0]:\n",
    "                      aux = ''\n",
    "                    elif '@' in aux:\n",
    "                      counter = aux.count('@')\n",
    "                      for k in range(counter): \n",
    "                        aux = re.sub(r'\\@', 'a', aux)\n",
    "\n",
    "                  if aux != '':\n",
    "                    final_list.append(aux)\n",
    "\n",
    "                  final_string = \" \".join(final_list)\n",
    "                  array[line][j] = final_string\n",
    "\n",
    "\n",
    "  #Creating a dictionary and final dataframe\n",
    "\n",
    "    # Header\n",
    "    \n",
    "    keys = []\n",
    "    count = 0\n",
    "    max = columns\n",
    "\n",
    "    for k in range(0, max, 2):\n",
    "      if k == 0:\n",
    "        keys.append('sentence%s' % str(k))\n",
    "        count = count + 1\n",
    "\n",
    "      elif k != 0 and (k + 2) != max:\n",
    "        decrement_target = k - (count + 1)\n",
    "        decrement_sentence = k - count\n",
    "        keys.append('target%s' % str(decrement_target))\n",
    "        keys.append('sentence%s' % str(decrement_sentence))\n",
    "        count = count + 1\n",
    "\n",
    "      elif (k + 2) == max:\n",
    "        keys.append('target%s' % str(decrement_sentence))\n",
    "        decrement_target = k - (count + 1)\n",
    "        decrement_sentence = k - count\n",
    "        keys.append('sentence%s' % str(decrement_sentence))\n",
    "        keys.append('target%s' % str(decrement_sentence))\n",
    "\n",
    "    if columns == 1:\n",
    "      dictionary = {keys[item]: array[:,item] for item in range(0,(columns),1)}\n",
    "\n",
    "    if columns != 1:\n",
    "      dictionary = {keys[item]: array[:,item] for item in range(0,(columns-1),1)}\n",
    "\n",
    "    df = pd.DataFrame(dictionary)\n",
    "    dataset = pd.concat([dataframe_first, df, target], axis = 1)\n",
    "\n",
    "    '''\n",
    "    Save the file\n",
    "    '''\n",
    "    \n",
    "    dataset.to_excel(file_name) \n",
    "    print('Well done!!')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VL64TzkgLXlX"
   },
   "outputs": [],
   "source": [
    "# Bag-of-Words\n",
    "\n",
    "def pre_process_bag_of_words(dataframe, file_name, language):\n",
    "\n",
    "    file_name = \"%s.xlsx\" % file_name\n",
    "    target = dataframe['target']\n",
    "    del dataframe['target']\n",
    "    lines, columns = dataframe.shape\n",
    "    stopwords = nltk.corpus.stopwords.words(language) # 'portuguese'\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    if columns != 1:\n",
    "      dataframe_first = dataframe['sentence']\n",
    "      del dataframe['sentence']\n",
    "      array = np.array(dataframe)\n",
    "\n",
    "      for j in range(0,(columns-1),2):\n",
    "        for line in range(lines):\n",
    "          string = str(array[line][j])\n",
    "          if string == 'nan':\n",
    "            string = '??'\n",
    "            array[line][j] = string\n",
    "\n",
    "          if string != 'nan' and string != '??':\n",
    "            if array[line][j] != '':\n",
    "              string = str(array[line][j])\n",
    "              string = string.lower()\n",
    "              array_tokens = string.split()\n",
    "              final_list = []\n",
    "              aux = []\n",
    "          \n",
    "            for i in range(len(array_tokens)):\n",
    "              if len(array_tokens) != 0:\n",
    "                aux = str(array_tokens[i])\n",
    "                hashtag = 0\n",
    "                if aux != '' and aux != '??':\n",
    "                  # it removes URLs      \n",
    "                  aux_URL = re.sub(r'(https?:(\\/\\/))?(www\\.)?[a-zA-Z0–9@:%._\\+~#=]{1,100}\\.[a-zA-Z0–9]{2,15}\\/?\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', aux)\n",
    "                  if aux_URL != aux:\n",
    "                    aux = ''\n",
    "                  else:\n",
    "                    aux = aux_URL\n",
    "\n",
    "                  # it removes emojis - https://pypi.org/project/demoji/\n",
    "                  if aux != '' and aux != '??':\n",
    "                    a = demoji.findall(aux)\n",
    "                    size = len(aux)\n",
    "                    if a != '{}': # if there are emojis\n",
    "                      list_emojis = [] \n",
    "                      for key in a.keys():\n",
    "                        aux = re.sub(key, '', aux)\n",
    "                      if aux == '':\n",
    "                        array_tokens[i] = aux\n",
    "\n",
    "                  # it removes '#'\n",
    "                  if '#' in aux: \n",
    "                    for char in aux:\n",
    "                      aux = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+', '', aux)\n",
    "                    aux = \"#%s\" % aux\n",
    "                    final_list.append(aux)\n",
    "                    hashtag = 1\n",
    "\n",
    "                  # it removes mentions\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    if '@' == aux[0]:\n",
    "                      aux = ''\n",
    "                    elif '@' in aux:\n",
    "                      counter = aux.count('@')\n",
    "                      for k in range(counter): \n",
    "                        aux = re.sub(r'\\@', 'a', aux)\n",
    "\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    # remove punctuation\n",
    "                    for char in aux:\n",
    "                      aux = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+', '', aux)\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    # remove stopwords\n",
    "                    b = 0\n",
    "                    while aux != '' and b < (len(stopwords)):\n",
    "                      if aux == stopwords[b]:\n",
    "                        aux = ''\n",
    "                        b = b + 1\n",
    "                      else: b = b + 1\n",
    "\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    aux = stemmer.stem(aux)                    \n",
    "                    final_list.append(aux)\n",
    "\n",
    "                  final_string = \" \".join(final_list)\n",
    "                  array[line][j] = final_string\n",
    "\n",
    "    if columns == 1:\n",
    "      dataframe_first = dataframe['sentence']\n",
    "      array = np.array(dataframe)\n",
    "\n",
    "      for j in range(0,(columns),2):\n",
    "        for line in range(lines):\n",
    "          string = str(array[line][j])\n",
    "\n",
    "          if string == 'nan':\n",
    "            string = '??'\n",
    "            array[line][j] = string\n",
    "\n",
    "          if string != 'nan' and string != '??':\n",
    "            if array[line][j] != '':\n",
    "              string = str(array[line][j])\n",
    "              string = string.lower()\n",
    "              array_tokens = string.split()\n",
    "              final_list = []\n",
    "              aux = []\n",
    "          \n",
    "            for i in range(len(array_tokens)):\n",
    "              if len(array_tokens) != 0:\n",
    "                aux = str(array_tokens[i])\n",
    "                hashtag = 0\n",
    "\n",
    "                if aux != '' and aux != '??':\n",
    "                  # it removes URLs      \n",
    "                  aux_URL = re.sub(r'(https?:(\\/\\/))?(www\\.)?[a-zA-Z0–9@:%._\\+~#=]{1,100}\\.[a-zA-Z0–9]{2,15}\\/?\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', aux)\n",
    "                  if aux_URL != aux:\n",
    "                    aux = ''\n",
    "                  else:\n",
    "                    aux = aux_URL\n",
    "\n",
    "                  # it removes emojis - https://pypi.org/project/demoji/\n",
    "                  if aux != '' and aux != '??':\n",
    "                    a = demoji.findall(aux)\n",
    "                    size = len(aux)\n",
    "                    if a != '{}': # if there are emojis\n",
    "                      list_emojis = [] \n",
    "                      for key in a.keys():\n",
    "                        aux = re.sub(key, '', aux)\n",
    "                      if aux == '':\n",
    "                        array_tokens[i] = aux\n",
    "\n",
    "                  # it removes '#'\n",
    "                  if '#' in aux: \n",
    "                    for char in aux:\n",
    "                      aux = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+', '', aux)\n",
    "                    aux = \"#%s\" % aux\n",
    "                    final_list.append(aux)\n",
    "                    hashtag = 1\n",
    "\n",
    "                  # it removes mentions\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    if '@' == aux[0]:\n",
    "                      aux = ''\n",
    "                    elif '@' in aux:\n",
    "                      counter = aux.count('@')\n",
    "                      for k in range(counter): \n",
    "                        aux = re.sub(r'\\@', 'a', aux)\n",
    "\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    # remove punctuation\n",
    "                    for char in aux:\n",
    "                      aux = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+', '', aux)\n",
    "\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    # remove stopwords\n",
    "                    b = 0\n",
    "                    while aux != '' and b < (len(stopwords)):\n",
    "                      if aux == stopwords[b]:\n",
    "                        aux = ''\n",
    "                        b = b + 1\n",
    "                      else: b = b + 1\n",
    "\n",
    "                  if aux != '' and aux != '??' and hashtag == 0:\n",
    "                    aux = stemmer.stem(aux)                    \n",
    "                    final_list.append(aux)\n",
    "\n",
    "                  final_string = \" \".join(final_list)\n",
    "                  array[line][j] = final_string\n",
    "\n",
    "  #Creating a dictionary and final dataframe\n",
    "\n",
    "    # Header\n",
    "    keys = []\n",
    "    count = 0\n",
    "    max = columns\n",
    "\n",
    "    for k in range(0, max, 2):\n",
    "\n",
    "      if k == 0:\n",
    "        keys.append('sentence%s' % str(k))\n",
    "        count = count + 1\n",
    "\n",
    "      elif k != 0 and (k + 2) != max:\n",
    "        decrement_target = k - (count + 1)\n",
    "        decrement_sentence = k - count\n",
    "        keys.append('target%s' % str(decrement_target))\n",
    "        keys.append('sentence%s' % str(decrement_sentence))\n",
    "        count = count + 1\n",
    "\n",
    "      elif (k + 2) == max:\n",
    "        keys.append('target%s' % str(decrement_sentence))\n",
    "        decrement_target = k - (count + 1)\n",
    "        decrement_sentence = k - count\n",
    "        keys.append('sentence%s' % str(decrement_sentence))\n",
    "        keys.append('target%s' % str(decrement_sentence))\n",
    "\n",
    "    # Dictionary and dataframe\n",
    "    if columns == 1:\n",
    "      dictionary = {keys[item]: array[:,item] for item in range(0,(columns),1)}\n",
    "\n",
    "    if columns != 1:\n",
    "      dictionary = {keys[item]: array[:,item] for item in range(0,(columns-1),1)}\n",
    "      \n",
    "    df = pd.DataFrame(dictionary)\n",
    "    dataset = pd.concat([dataframe_first, df, target], axis = 1)\n",
    "\n",
    "    '''\n",
    "    Save the file\n",
    "    '''\n",
    "    dataset.to_excel(file_name)\n",
    "    print('Well done!!')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0BYq55rFuay"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_Enu-8GT1i2"
   },
   "outputs": [],
   "source": [
    "# Import file and create DataFrame\n",
    "\n",
    "df_train = pd.read_excel('train.xlsx')\n",
    "df_test = pd.read_excel('test.xlsx')\n",
    "\n",
    "# BERT\n",
    "\n",
    "# train\n",
    "name_train_bert = input('Please, type a file name without spaces or special characters, and without extension (Train BERT):')\n",
    "final_dataset_train_bert = pre_process_BERT(df_train,name_train_bert)\n",
    "\n",
    "# test\n",
    "name_test_bert = input('Please, type a file name without spaces or special characters, and without extension (Test BERT):')\n",
    "final_dataset_test_bert = pre_process_BERT(df_test,name_test_bert)\n",
    "\n",
    "\n",
    "# Bag-of-words\n",
    "\n",
    "lang = input('Please, choose your language (lowercase only):')\n",
    "\n",
    "# train\n",
    "name_train_bag = input('Please, type a file name without spaces or special characters, and without extension (Train Bag-Of-Words):')\n",
    "final_dataset_train_bag = pre_process_bag_of_words(df_train,name_train_bag,lang)\n",
    "\n",
    "# test\n",
    "name_test_bag = input('Please, type a file name without spaces or special characters, and without extension (Test Bag-Of-Words):')\n",
    "final_dataset_test_bag = pre_process_bag_of_words(df_test,name_test_bag,lang)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Pre_processing_Tweets_with_Python.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
