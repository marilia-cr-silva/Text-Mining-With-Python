{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Mining-Twitter-with-Python.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soDJmzzyrWTM"
      },
      "source": [
        "# Mining Twitter with Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5drHKn_ZrWTO"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import tweepy as tweepy # https://github.com/tweepy/tweepy\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ku7qUy2rWTW"
      },
      "source": [
        "# Twitter developer credentials - the text file does not require typing the keys and tokens everytime you run the code\n",
        "\n",
        "with open('twitter_keys_tokens.txt', 'r') as tfile:\n",
        "    consumer_key = tfile.readline().strip('\\n')\n",
        "    consumer_secret = tfile.readline().strip('\\n')\n",
        "    access_token = tfile.readline().strip('\\n')\n",
        "    access_token_secret = tfile.readline().strip('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqvg6UeJrWTc"
      },
      "source": [
        "# Authentication - keys and tokens\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0CSoEIY_GMI"
      },
      "source": [
        "# Search Query - The query discards retweets  http://docs.tweepy.org/en/latest/api.html#\n",
        "\n",
        "list_twitter_query = [\"#BolsonaroNaOnu -filter:retweets\", \"'Parabéns Presidente' -filter:retweets\",  \"'Assembleia Geral da ONU' -filter:retweets\", \"'MIL DÓLARES' -filter:retweets\",  \"Cristofobia -filter:retweets\",  \"#BolsonaroMentiroso -filter:retweets\", \"Bozo -filter:retweets\",  \"#BolsonaroOrgulhaOBrasil -filter:retweets\"]\n",
        "list_result_type = ['mixed', 'recent', 'popular']\n",
        "\n",
        "# Defining the search period (9-days window)\n",
        "#list_final_date = ['2020-09-22', '2020-09-23']\n",
        "number_tweets = '100'\n",
        "\n",
        "for j in range(len(list_result_type)):\n",
        "  for k in range(len(list_twitter_query)):\n",
        "\n",
        "  # \"Results\" get all the tweets and their metadata\n",
        "    results = api.search(list_twitter_query[k], lang = 'pt', since = '2020-09-22', until = '2020-09-24', result_type = list_result_type[j], count = number_tweets, tweet_mode = 'extended')\n",
        "    \n",
        "    for m in range(len(results)):\n",
        "  \n",
        "      aux = results[m]\n",
        "      json_string = json.dumps(aux._json, ensure_ascii=False)\n",
        "      dictionary = json.loads(json_string)\n",
        "\n",
        "      if 'extended_entities' in dictionary:\n",
        "        dictionary.pop('extended_entities')\n",
        "\n",
        "      if 'possibly_sensitive' in dictionary:\n",
        "        dictionary.pop('possibly_sensitive')\n",
        "\n",
        "      if 'retweeted_status' in dictionary:\n",
        "        dictionary.pop('retweeted_status')\n",
        "\n",
        "      if k == 0 and j == 0:\n",
        "        df = pd.DataFrame.from_dict(dictionary, orient='index')\n",
        "\n",
        "      else:\n",
        "        df_aux = pd.DataFrame.from_dict(dictionary, orient='index')\n",
        "        df = pd.concat([df,df_aux], axis = 1)\n",
        "\n",
        "df_transposed = df.transpose()\n",
        "df_transposed.index = range(len(df_transposed))\n",
        "df_transposed.to_excel(\"complete_output.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwPFTHy2jnqj"
      },
      "source": [
        "# [Appendix] Overall comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLFcp5BYjouq"
      },
      "source": [
        "The results have the format \"SearchResults\".\n",
        "However, this format does not allow operations similar to arrays, strings, dataframes, dictionaries, JSON, series.\n",
        "Thus, there are the following steps to make sure that the final file will provide a useful dataset with the text and metadata.\n",
        "\n",
        "1. Each row (status) contains an embedded JSON.\n",
        "Therefore, these rows will firstly be extracted to a JSON object, in order to perform further processes.\n",
        "\n",
        "2. JSON to dictionary.\n",
        "\n",
        "3. There is an error concerning the Tweepy library. The function does not gather the same data to all of the instances (tweets).\n",
        "Ergo, there is an intervention so that it is possible to concatenate these tweets into a dataframe.\n",
        "\n",
        "4. A dataframe is created and concatenated, in order to make easier the process of saving a spreadsheet (.xlsx).\n",
        "\n",
        "5. It is necessary to transpose the dataframe, so that the instances become the rows and the metadata become the columns.\n",
        "This change makes the dataframe more friendly to preprocessing/ML algorithms.\n",
        "\n",
        "6. An .xlsx file keeps the original format of the dataframe.\n",
        "For example, a .csv file would compromise the content of the tweets."
      ]
    }
  ]
}