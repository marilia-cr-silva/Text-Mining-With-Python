{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3XO86iKyh7Q"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu9YCicx65Wg"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "!pip install ktrain\n",
    "import keras\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxfkD70vu1iK"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxfHgzeN7fVu"
   },
   "outputs": [],
   "source": [
    "# test dataset - tweets - the sentences are merged into a single tweet\n",
    "\n",
    "def dataframe_full_text(dataframe):\n",
    "  del dataframe['Unnamed: 0']\n",
    "  del dataframe['sentence']\n",
    "  lines, columns = dataframe.shape\n",
    "\n",
    "  if columns == 2:\n",
    "    df_final_test = dataframe\n",
    "    df_final_test.columns = ['sentence','target']\n",
    "\n",
    "  if columns > 2:\n",
    "    target = dataframe['target']\n",
    "    del dataframe['target']\n",
    "    lines, columns = dataframe.shape\n",
    "    array_test = np.array(dataframe)\n",
    "    array_empty = []\n",
    "    \n",
    "    for line in range(lines):\n",
    "      final_list = []\n",
    "      for column in range(0,columns-1,2):\n",
    "        string = str(array_test[line][column])\n",
    "        if string != '' and string != '??' and string != 'nan' and string != 'NaN':\n",
    "          if '1.0' in string:\n",
    "            string = re.sub(r'1.0','1',string)\n",
    "          if '2.0' in string:\n",
    "            string = re.sub(r'2.0','2',string)\n",
    "          if '3.0' in string:\n",
    "            string = re.sub(r'3.0','3',string)\n",
    "          if '4.0' in string:\n",
    "            string = re.sub(r'4.0','4',string)\n",
    "          if '5.0' in string:\n",
    "            string = re.sub(r'5.0','5',string)\n",
    "          final_list.append(string)\n",
    "      final_string = ' '.join(final_list)\n",
    "      array_empty.append(final_string)\n",
    "\n",
    "    df_string_concat = pd.DataFrame(array_empty)\n",
    "    df_final_test = pd.concat((df_string_concat, target), axis = 1)\n",
    "    df_final_test.columns = ['sentence','target']\n",
    "\n",
    "  return df_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMt1LvN4iTxR"
   },
   "outputs": [],
   "source": [
    "# test dataset - sentences - the dataset comprises only sentences\n",
    "\n",
    "def dataframe_sentences(dataframe):\n",
    "  del dataframe['Unnamed: 0']\n",
    "  del dataframe['sentence']\n",
    "  lines, columns = dataframe.shape\n",
    "\n",
    "  if columns == 2:\n",
    "    df = dataframe\n",
    "\n",
    "  if columns > 2:\n",
    "    del dataframe['target']\n",
    "    lines, columns = dataframe.shape\n",
    "    array_test = np.array(dataframe)\n",
    "    sentence_target_list = []\n",
    "    \n",
    "    for line in range(lines):\n",
    "      for column in range(0,columns):\n",
    "        string = str(array_test[line][column])\n",
    "        if '1.0' in string:\n",
    "          string = re.sub(r'1.0','1',string)\n",
    "        if '2.0' in string:\n",
    "          string = re.sub(r'2.0','2',string)\n",
    "        if '3.0' in string:\n",
    "          string = re.sub(r'3.0','3',string)\n",
    "        if '4.0' in string:\n",
    "          string = re.sub(r'4.0','4',string)\n",
    "        if '5.0' in string:\n",
    "          string = re.sub(r'5.0','5',string)\n",
    "\n",
    "        if string != '' and string != '??' and string != 'nan' and string != 'NaN':\n",
    "          if ((len(sentence_target_list) != 0) and ((sentence_target_list[-1] == '1') or (sentence_target_list[-1] == '2') or (sentence_target_list[-1] == '3') or (sentence_target_list[-1] == '4') or (sentence_target_list[-1] == '5')) and ((string != '1') or (string != '2') or (string != '3') or (string != '4') or (string != '5'))):\n",
    "            sentence_target_list.append(string)\n",
    "          if ((len(sentence_target_list) != 0) and ((sentence_target_list[-1] != '1') or (sentence_target_list[-1] != '2') or (sentence_target_list[-1] != '3') or (sentence_target_list[-1] != '4') or (sentence_target_list[-1] != '5')) and ((string == '1') or (string == '2') or (string == '3') or (string == '4') or (string == '5'))):\n",
    "            sentence_target_list.append(string)\n",
    "          if len(sentence_target_list) == 0:\n",
    "            sentence_target_list.append(string)\n",
    "\n",
    "    length = len(sentence_target_list)\n",
    "    lines = int(length/2)\n",
    "    columns = 2\n",
    "    list_to_array = np.array(sentence_target_list)\n",
    "    final_array = np.reshape(list_to_array, (lines, columns))\n",
    "    df = pd.DataFrame(final_array)\n",
    "  df.columns = ['sentence','target']\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wwW_6wXeHzK"
   },
   "outputs": [],
   "source": [
    "# classification - one-hot\n",
    "\n",
    "def one_hot(dataframe):\n",
    "  list_01 = [1,0,0,0,0]\n",
    "  list_02 = [0,1,0,0,0]\n",
    "  list_03 = [0,0,1,0,0]\n",
    "  list_04 = [0,0,0,1,0]\n",
    "  list_05 = [0,0,0,0,1]\n",
    "  lines, columns = dataframe.shape\n",
    "  df_array = np.array(dataframe)\n",
    "  one_hot_vectors = []\n",
    "\n",
    "  for i in range(lines):\n",
    "    if df_array[i][1] == 1 or df_array[i][1] == 1.0 or df_array[i][1] == '1' or df_array[i][1] == '1.0':\n",
    "      one_hot_vectors.append(list_01)\n",
    "    elif df_array[i][1] == 2 or df_array[i][1] == 2.0 or df_array[i][1] == '2' or df_array[i][1] == '2.0':\n",
    "      one_hot_vectors.append(list_02)\n",
    "    elif df_array[i][1] == 3 or df_array[i][1] == 3.0 or df_array[i][1] == '3' or df_array[i][1] == '3.0':\n",
    "      one_hot_vectors.append(list_03)\n",
    "    elif df_array[i][1] == 4 or df_array[i][1] == 4.0 or df_array[i][1] == '4' or df_array[i][1] == '4.0':\n",
    "      one_hot_vectors.append(list_04)\n",
    "    elif df_array[i][1] == 5 or df_array[i][1] == 5.0 or df_array[i][1] == '5' or df_array[i][1] == '5.0':\n",
    "      one_hot_vectors.append(list_05)\n",
    "\n",
    "  one_hot_array = np.array(one_hot_vectors).reshape(len(one_hot_vectors),5)\n",
    "  one_hot_dataframe = pd.DataFrame(one_hot_array)\n",
    "  del dataframe['target']\n",
    "  df_one_hot = pd.concat((dataframe,one_hot_dataframe), axis = 1)\n",
    "  df_one_hot.columns = ['sentence', '01','02','03','04','05']\n",
    "\n",
    "  return df_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWkA-7EEsnmC"
   },
   "source": [
    "# Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VyGUODFD9qw"
   },
   "outputs": [],
   "source": [
    "# Loading files to dataframe\n",
    "\n",
    "# train dataset\n",
    "df_train = one_hot(dataframe_sentences(pd.read_excel('train_bag.xlsx')))\n",
    "df_train_stemmer = one_hot(dataframe_sentences(pd.read_excel('train_bag_stemmer.xlsx')))\n",
    "\n",
    "# test dataset\n",
    "df_test_sentence = one_hot(dataframe_sentences(pd.read_excel('test_bag.xlsx')))\n",
    "df_test_sentence_stemmer = one_hot(dataframe_sentences(pd.read_excel('test_bag_stemmer.xlsx')))\n",
    "df_test_tweet = one_hot(dataframe_full_text(pd.read_excel('test_bag.xlsx')))\n",
    "df_test_tweet_stemmer = one_hot(dataframe_full_text(pd.read_excel('test_bag_stemmer.xlsx')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhEYYJc0KY8e"
   },
   "source": [
    "# Test 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4nqBcIKd-Vxu",
    "outputId": "b7506dcb-c09e-4fbb-f4a3-7a147de6a515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: pt\n",
      "Word Counts: 2413\n",
      "Nrows: 1170\n",
      "1170 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 16\n",
      "\t99percentile : 23\n",
      "x_train shape: (1170,64)\n",
      "y_train shape: (1170, 5)\n",
      "Is Multi-Label? False\n",
      "218 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 3\n",
      "\t95percentile : 9\n",
      "\t99percentile : 16\n",
      "x_test shape: (218,64)\n",
      "y_test shape: (218, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create vectors https://github.com/amaiya/ktrain/blob/master/ktrain/text/data.py\n",
    "# random_state = 42, so that every time that the algorithm is run it leads to the same results\n",
    "# high number of max_features - the entire corpus was included\n",
    "# the maxlen was not a huge concern - tweets are short (limited by characters)\n",
    "\n",
    "(x_train_00, y_train_00), (x_test_sentence, y_test_sentence), preproc = text.texts_from_df(df_train, text_column = 'sentence',\n",
    "                                                                                      label_columns=['01','02','03','04','05'],\n",
    "                                                                                      maxlen=64, \n",
    "                                                                                      max_features = 3000,\n",
    "                                                                                      preprocess_mode = 'standard',\n",
    "                                                                                      lang = None,\n",
    "                                                                                      ngram_range=1,\n",
    "                                                                                      val_df = df_test_sentence,\n",
    "                                                                                      random_state = 42\n",
    "                                                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yWOTVj7LEOT_",
    "outputId": "f1ce0dbb-b101-434a-dc92-a197ff00f7f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 64\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-1170\n",
      "computing log-count ratios...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "learn_bag_test_01 = ktrain.get_learner(text.text_classifier('nbsvm', (x_train_00, y_train_00) , preproc=preproc), \n",
    "                             train_data=(x_train_00, y_train_00), \n",
    "                             val_data=(x_test_sentence, y_test_sentence), batch_size = 10\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nY0Wh1r8EN1v",
    "outputId": "65c184b8-c92f-4777-f1a2-ebe32bf58b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.01...\n",
      "Epoch 1/20\n",
      "234/234 [==============================] - 1s 2ms/step - loss: 1.5319 - accuracy: 0.3060 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 2/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 1.3507 - accuracy: 0.4427 - val_loss: nan - val_accuracy: 0.1606\n",
      "Epoch 3/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 1.2130 - accuracy: 0.4974 - val_loss: nan - val_accuracy: 0.1697\n",
      "Epoch 4/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 1.1195 - accuracy: 0.5222 - val_loss: nan - val_accuracy: 0.1927\n",
      "Epoch 5/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 1.0489 - accuracy: 0.5598 - val_loss: nan - val_accuracy: 0.1881\n",
      "Epoch 6/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.9946 - accuracy: 0.5769 - val_loss: nan - val_accuracy: 0.1927\n",
      "Epoch 7/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.9496 - accuracy: 0.5940 - val_loss: nan - val_accuracy: 0.1927\n",
      "Epoch 8/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.9108 - accuracy: 0.6094 - val_loss: nan - val_accuracy: 0.1972\n",
      "Epoch 9/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.8761 - accuracy: 0.6325 - val_loss: nan - val_accuracy: 0.2110\n",
      "Epoch 10/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.8503 - accuracy: 0.6325 - val_loss: nan - val_accuracy: 0.2018\n",
      "Epoch 11/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.8215 - accuracy: 0.6410 - val_loss: nan - val_accuracy: 0.2110\n",
      "Epoch 12/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7952 - accuracy: 0.6410 - val_loss: nan - val_accuracy: 0.2110\n",
      "Epoch 13/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7668 - accuracy: 0.6658 - val_loss: nan - val_accuracy: 0.2064\n",
      "Epoch 14/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7475 - accuracy: 0.6718 - val_loss: nan - val_accuracy: 0.2064\n",
      "Epoch 15/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.6718 - val_loss: nan - val_accuracy: 0.2064\n",
      "Epoch 16/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7235 - accuracy: 0.6795 - val_loss: nan - val_accuracy: 0.2110\n",
      "Epoch 17/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7150 - accuracy: 0.6897 - val_loss: nan - val_accuracy: 0.2064\n",
      "Epoch 18/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7083 - accuracy: 0.6897 - val_loss: nan - val_accuracy: 0.2018\n",
      "Epoch 19/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7035 - accuracy: 0.6923 - val_loss: nan - val_accuracy: 0.2018\n",
      "Epoch 20/20\n",
      "234/234 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.6923 - val_loss: nan - val_accuracy: 0.2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2c51632898>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_bag_test_01.fit_onecycle(0.01,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "76993Zx5zAzd",
    "outputId": "dbd2cba8-c376-476a-e192-1218081856cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.34       218\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.20       218\n",
      "   macro avg       0.20      0.04      0.07       218\n",
      "weighted avg       1.00      0.20      0.34       218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44, 55, 36, 53, 30],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_bag_test_01.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7a3jVqrjacw"
   },
   "source": [
    "# Test 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r5qEYhDELDCU",
    "outputId": "2a7a9d84-f8a2-41e0-a39d-1ebe903f4b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: pt\n",
      "Word Counts: 2413\n",
      "Nrows: 1170\n",
      "1170 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 16\n",
      "\t99percentile : 23\n",
      "x_train shape: (1170,64)\n",
      "y_train shape: (1170, 5)\n",
      "Is Multi-Label? False\n",
      "218 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 8\n",
      "\t95percentile : 17\n",
      "\t99percentile : 24\n",
      "x_test shape: (218,64)\n",
      "y_test shape: (218, 5)\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 64\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-1170\n",
      "computing log-count ratios...\n",
      "done.\n",
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.01...\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.5419 - accuracy: 0.3000 - val_loss: 1.3060 - val_accuracy: 0.6881\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.3996 - accuracy: 0.4402 - val_loss: 1.0609 - val_accuracy: 0.7294\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.2709 - accuracy: 0.4769 - val_loss: 0.9071 - val_accuracy: 0.7248\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1726 - accuracy: 0.5120 - val_loss: 0.8457 - val_accuracy: 0.7431\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1004 - accuracy: 0.5453 - val_loss: 0.8183 - val_accuracy: 0.7385\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0422 - accuracy: 0.5658 - val_loss: 0.8211 - val_accuracy: 0.7248\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9958 - accuracy: 0.5803 - val_loss: 0.8248 - val_accuracy: 0.7064\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9552 - accuracy: 0.6103 - val_loss: 0.8604 - val_accuracy: 0.6927\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9201 - accuracy: 0.6051 - val_loss: 0.8729 - val_accuracy: 0.6927\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8888 - accuracy: 0.6231 - val_loss: 0.9215 - val_accuracy: 0.6560\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8627 - accuracy: 0.6316 - val_loss: 0.9604 - val_accuracy: 0.6651\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8339 - accuracy: 0.6444 - val_loss: 0.9995 - val_accuracy: 0.6330\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8099 - accuracy: 0.6632 - val_loss: 1.0274 - val_accuracy: 0.6239\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7920 - accuracy: 0.6641 - val_loss: 1.0591 - val_accuracy: 0.6284\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7786 - accuracy: 0.6692 - val_loss: 1.0821 - val_accuracy: 0.6284\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7684 - accuracy: 0.6726 - val_loss: 1.1026 - val_accuracy: 0.6284\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7606 - accuracy: 0.6735 - val_loss: 1.1199 - val_accuracy: 0.6284\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7546 - accuracy: 0.6778 - val_loss: 1.1325 - val_accuracy: 0.6284\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7502 - accuracy: 0.6778 - val_loss: 1.1404 - val_accuracy: 0.6284\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7474 - accuracy: 0.6778 - val_loss: 1.1433 - val_accuracy: 0.6193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      1.00      0.10         1\n",
      "           1       0.82      0.70      0.76        80\n",
      "           2       0.33      0.41      0.37        17\n",
      "           3       0.68      0.57      0.62        95\n",
      "           4       0.57      0.68      0.62        25\n",
      "\n",
      "    accuracy                           0.62       218\n",
      "   macro avg       0.49      0.67      0.49       218\n",
      "weighted avg       0.69      0.62      0.65       218\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0],\n",
       "       [ 8, 56,  2, 14,  0],\n",
       "       [ 1,  2,  7,  6,  1],\n",
       "       [10,  8, 11, 54, 12],\n",
       "       [ 0,  2,  1,  5, 17]])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_01, y_train_01), (x_test_tweet, y_test_tweet), preproc01 = text.texts_from_df(df_train, text_column='sentence',\n",
    "                                                                                    label_columns=['01','02','03','04','05'],\n",
    "                                                                                    maxlen=64, \n",
    "                                                                                    max_features=3000,\n",
    "                                                                                    preprocess_mode='standard',\n",
    "                                                                                    lang=None,\n",
    "                                                                                    ngram_range=1,\n",
    "                                                                                    val_df = df_test_tweet,\n",
    "                                                                                    random_state = 42\n",
    "                                                                                    )\n",
    "\n",
    "learn_bag_test_02 = ktrain.get_learner(text.text_classifier('nbsvm', (x_train_01, y_train_01) , preproc=preproc01), \n",
    "                                train_data=(x_train_01, y_train_01), \n",
    "                                val_data=(x_test_tweet, y_test_tweet), batch_size = 10\n",
    "                                )\n",
    "\n",
    "learn_bag_test_02.fit_onecycle(0.01,20)\n",
    "\n",
    "learn_bag_test_02.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0BYq55rFuay"
   },
   "source": [
    "# Test 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5x4nDwBhmqqf",
    "outputId": "98a5183d-41d0-4652-8878-c5dd465d9d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: pt\n",
      "Word Counts: 1852\n",
      "Nrows: 1170\n",
      "1170 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 15\n",
      "\t99percentile : 22\n",
      "x_train shape: (1170,64)\n",
      "y_train shape: (1170, 5)\n",
      "Is Multi-Label? False\n",
      "218 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 4\n",
      "\t95percentile : 10\n",
      "\t99percentile : 16\n",
      "x_test shape: (218,64)\n",
      "y_test shape: (218, 5)\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 64\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-1170\n",
      "computing log-count ratios...\n",
      "done.\n",
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.01...\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.5593 - accuracy: 0.2744 - val_loss: nan - val_accuracy: 0.2248\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.4084 - accuracy: 0.4299 - val_loss: nan - val_accuracy: 0.1468\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.2762 - accuracy: 0.4692 - val_loss: nan - val_accuracy: 0.1376\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1848 - accuracy: 0.4923 - val_loss: nan - val_accuracy: 0.1239\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1206 - accuracy: 0.5111 - val_loss: nan - val_accuracy: 0.1284\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0734 - accuracy: 0.5188 - val_loss: nan - val_accuracy: 0.1330\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0369 - accuracy: 0.5299 - val_loss: nan - val_accuracy: 0.1376\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0061 - accuracy: 0.5462 - val_loss: nan - val_accuracy: 0.1422\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9825 - accuracy: 0.5487 - val_loss: nan - val_accuracy: 0.1468\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9596 - accuracy: 0.5547 - val_loss: nan - val_accuracy: 0.1468\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9424 - accuracy: 0.5598 - val_loss: nan - val_accuracy: 0.1376\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9249 - accuracy: 0.5709 - val_loss: nan - val_accuracy: 0.1468\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9101 - accuracy: 0.5718 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8987 - accuracy: 0.5778 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8891 - accuracy: 0.5821 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8820 - accuracy: 0.5889 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8765 - accuracy: 0.5889 - val_loss: nan - val_accuracy: 0.1514\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8713 - accuracy: 0.5940 - val_loss: nan - val_accuracy: 0.1468\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8677 - accuracy: 0.5974 - val_loss: nan - val_accuracy: 0.1560\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8649 - accuracy: 0.6009 - val_loss: nan - val_accuracy: 0.1560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       218\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.16       218\n",
      "   macro avg       0.20      0.03      0.05       218\n",
      "weighted avg       1.00      0.16      0.27       218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[34, 68, 26, 59, 31],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_stemmer_00, y_train_stemmer_00), (x_test_sentence_stemmer, y_test_sentence_stemmer), preproc_stemmer_00 = text.texts_from_df(df_train_stemmer, text_column='sentence',\n",
    "                                                                                                                                    label_columns=['01','02','03','04','05'],\n",
    "                                                                                                                                    maxlen=64, \n",
    "                                                                                                                                    max_features=1000,\n",
    "                                                                                                                                    preprocess_mode='standard',\n",
    "                                                                                                                                    lang='pt',\n",
    "                                                                                                                                    ngram_range=1,\n",
    "                                                                                                                                    val_df = df_test_sentence_stemmer,\n",
    "                                                                                                                                    random_state = 42\n",
    "                                                                                                                                    )\n",
    "learn_bag_test_03 = ktrain.get_learner(text.text_classifier('nbsvm', (x_train_stemmer_00, y_train_stemmer_00) , preproc=preproc_stemmer_00), \n",
    "                                                          train_data=(x_train_stemmer_00, y_train_stemmer_00), \n",
    "                                                          val_data=(x_test_sentence_stemmer, y_test_sentence_stemmer), batch_size = 10\n",
    "                                                          )\n",
    "\n",
    "learn_bag_test_03.fit_onecycle(0.01,20)\n",
    "\n",
    "learn_bag_test_03.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_QmO6LCLVFt"
   },
   "source": [
    "# Test 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rhyt_mh9LcV7",
    "outputId": "6521a473-7e80-497a-8a45-2c7a1824284e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: pt\n",
      "Word Counts: 1852\n",
      "Nrows: 1170\n",
      "1170 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 15\n",
      "\t99percentile : 22\n",
      "x_train shape: (1170,64)\n",
      "y_train shape: (1170, 5)\n",
      "Is Multi-Label? False\n",
      "218 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 8\n",
      "\t95percentile : 19\n",
      "\t99percentile : 26\n",
      "x_test shape: (218,64)\n",
      "y_test shape: (218, 5)\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 64\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-1170\n",
      "computing log-count ratios...\n",
      "done.\n",
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.01...\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.5489 - accuracy: 0.2846 - val_loss: 1.2600 - val_accuracy: 0.6743\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.4013 - accuracy: 0.4188 - val_loss: 1.0192 - val_accuracy: 0.7294\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.2748 - accuracy: 0.4744 - val_loss: 0.8838 - val_accuracy: 0.7202\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1850 - accuracy: 0.4966 - val_loss: 0.8247 - val_accuracy: 0.7477\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1224 - accuracy: 0.5094 - val_loss: 0.8013 - val_accuracy: 0.7523\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0747 - accuracy: 0.5265 - val_loss: 0.7985 - val_accuracy: 0.7385\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0377 - accuracy: 0.5359 - val_loss: 0.8139 - val_accuracy: 0.7202\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0067 - accuracy: 0.5419 - val_loss: 0.8232 - val_accuracy: 0.7156\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9826 - accuracy: 0.5504 - val_loss: 0.8461 - val_accuracy: 0.7110\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9608 - accuracy: 0.5556 - val_loss: 0.8695 - val_accuracy: 0.7156\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9430 - accuracy: 0.5573 - val_loss: 0.9055 - val_accuracy: 0.7156\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9241 - accuracy: 0.5692 - val_loss: 0.9229 - val_accuracy: 0.7156\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9098 - accuracy: 0.5744 - val_loss: 0.9522 - val_accuracy: 0.6972\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8987 - accuracy: 0.5735 - val_loss: 0.9637 - val_accuracy: 0.7018\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8895 - accuracy: 0.5778 - val_loss: 0.9808 - val_accuracy: 0.7018\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8817 - accuracy: 0.5915 - val_loss: 0.9942 - val_accuracy: 0.7018\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8762 - accuracy: 0.5915 - val_loss: 1.0046 - val_accuracy: 0.6972\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8712 - accuracy: 0.5949 - val_loss: 1.0130 - val_accuracy: 0.6972\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8677 - accuracy: 0.5923 - val_loss: 1.0182 - val_accuracy: 0.6972\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8648 - accuracy: 0.5983 - val_loss: 1.0195 - val_accuracy: 0.6927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      1.00      0.14         1\n",
      "           1       0.81      0.74      0.77        80\n",
      "           2       0.38      0.29      0.33        17\n",
      "           3       0.72      0.71      0.71        95\n",
      "           4       0.73      0.76      0.75        25\n",
      "\n",
      "    accuracy                           0.69       218\n",
      "   macro avg       0.54      0.70      0.54       218\n",
      "weighted avg       0.72      0.69      0.71       218\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0],\n",
       "       [ 5, 59,  2, 14,  0],\n",
       "       [ 1,  3,  5,  8,  0],\n",
       "       [ 6,  9,  6, 67,  7],\n",
       "       [ 0,  2,  0,  4, 19]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_stemmer_01, y_train_stemmer_01), (x_test_tweet_stemmer, y_test_tweet_stemmer), preproc_stemmer_01 = text.texts_from_df(df_train_stemmer, text_column='sentence',\n",
    "                                                                                                                              label_columns=['01','02','03','04','05'],\n",
    "                                                                                                                              maxlen=64, \n",
    "                                                                                                                              max_features=1000,\n",
    "                                                                                                                              preprocess_mode='standard',\n",
    "                                                                                                                              lang='pt',\n",
    "                                                                                                                              ngram_range=1,\n",
    "                                                                                                                              val_df = df_test_tweet_stemmer,\n",
    "                                                                                                                              random_state = 42\n",
    "                                                                                                                             )\n",
    "\n",
    "learn_bag_test_04 = ktrain.get_learner(text.text_classifier('nbsvm', (x_train_stemmer_01, y_train_stemmer_01) , preproc=preproc_stemmer_01), \n",
    "                                    train_data=(x_train_stemmer_01, y_train_stemmer_01), \n",
    "                                    val_data=(x_test_tweet_stemmer, y_test_tweet_stemmer), batch_size = 10\n",
    "                                    )\n",
    "\n",
    "learn_bag_test_04.fit_onecycle(0.01,20)\n",
    "\n",
    "learn_bag_test_04.validate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classifying_tweets_with_Bag_of_words_NBSVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
